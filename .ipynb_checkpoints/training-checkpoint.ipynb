{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'data/models/test7/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load recorded images and steering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_camera_offset = -0.27\n",
    "left_camera_offset = 0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(data_dir, discard_prob):\n",
    "    df = pd.read_csv(data_dir+'driving_log.csv')\n",
    "    images = []\n",
    "    measurements = []\n",
    "\n",
    "    def fetch_image(data_dir, source_path):\n",
    "        filename = source_path.split('/')[-1]\n",
    "        current_path = data_dir + 'IMG/' + filename\n",
    "        return imageio.imread(current_path)\n",
    "\n",
    "    def append_image(image, measurement):\n",
    "        images.append(image)\n",
    "        measurements.append(measurement)\n",
    "\n",
    "        # flip image\n",
    "        image_flipped = np.fliplr(image)\n",
    "        measurement_flipped = -measurement\n",
    "        images.append(image_flipped)\n",
    "        measurements.append(measurement_flipped)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        measurement = float(row[3])\n",
    "        if measurement <= 0 and np.random.rand() < discard_prob:\n",
    "            continue\n",
    "            \n",
    "        center_image = fetch_image(data_dir, row[0])\n",
    "        append_image(center_image, measurement)\n",
    "        \n",
    "        left_image = fetch_image(data_dir, row[1])\n",
    "        append_image(left_image, measurement+left_camera_offset)\n",
    "        \n",
    "        right_image = fetch_image(data_dir, row[2])\n",
    "        append_image(right_image, measurement+right_camera_offset)\n",
    "        \n",
    "    return images, measurements\n",
    "    \n",
    "def load_from_dir(data_dir, discard_prob=0, load_cached=False):\n",
    "    if load_cached and os.path.exists(data_dir+'X_train.npy'):\n",
    "        X_train = np.load(data_dir+'X_train.npy')\n",
    "        y_train = np.load(data_dir+'y_train.npy')\n",
    "    else:\n",
    "        images, measurements = load_images(data_dir, discard_prob)\n",
    "        \n",
    "        X_train = np.array(images)\n",
    "        y_train = np.array(measurements)\n",
    "\n",
    "        np.save(data_dir+'X_train.npy', X_train)\n",
    "        np.save(data_dir+'y_train.npy', y_train)\n",
    "    \n",
    "    print('X_train:', X_train.shape)\n",
    "    print('y_train:', y_train.shape)\n",
    "    return X_train, y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Recorded data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sample data\n",
    "X_train_sample, y_train_sample = load_from_dir('data/sample_data/', discard_prob=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dirt road where vehicle messed up\n",
    "X_train_dirt, y_train_dirt = load_from_dir('data/recorded_data/lap1_dirt_bridge/', discard_prob=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Correcting car from out of bounds\n",
    "X_train_mis, y_train_mis = load_from_dir('data/recorded_data/lap1mistakes/', discard_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Just 1 Lap\n",
    "X_train_lap1, y_train_lap1 = load_from_dir('data/recorded_data/lap1/', discard_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 1 Lap in reverse\n",
    "X_train_rev, y_train_rev = load_from_dir('data/recorded_data/lap1reverse/', discard_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all the records into one training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = np.concatenate([X_train_sample, X_train_dirt, X_train_mis, X_train_lap1, X_train_rev])\n",
    "y_train_all = np.concatenate([y_train_sample, y_train_dirt, y_train_mis, y_train_lap1, y_train_rev])\n",
    "np.save('data/all/'+'X_train_all.npy', X_train_all)\n",
    "np.save('data/all/'+'y_train_all.npy', y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = np.load('data/all/'+'X_train_all.npy')\n",
    "y_train_all = np.load('data/all/'+'y_train_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_all.shape)\n",
    "print(y_train_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Lambda, Convolution2D, BatchNormalization, Input, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA Suggested network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x - 255.0 / 255.0, input_shape=(160,320,3)))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(keras.layers.Convolution2D(24, (5,5), strides=(2,2), activation='relu'))\n",
    "model.add(keras.layers.Convolution2D(36, (5,5), strides=(2,2), activation='relu'))\n",
    "model.add(keras.layers.Convolution2D(48, (5,5), strides=(2,2), activation='relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(keras.layers.Convolution2D(64, (3,3), strides=(1,1), activation='relu'))\n",
    "model.add(keras.layers.Convolution2D(64, (3,3), strides=(1,1), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_object = model.fit(X_train_all, y_train_all, validation_split=0.2, shuffle=True, epochs=7)\n",
    "model.save(model_dir+'nvidia_suggested_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_dir+'nvidia_suggested_model.h5')\n",
    "model.optimizer.lr.assign(0.005)\n",
    "model.fit(X_train_all, y_train_all, validation_split=0.2, shuffle=True, epochs=3)\n",
    "\n",
    "model.save(model_dir+'nvidia_suggested_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_dir+'nvidia_suggested_model_2.h5')\n",
    "model.optimizer.lr.assign(0.0001)\n",
    "model.fit(X_train_all, y_train_all, validation_split=0.2, shuffle=True, epochs=2)\n",
    "\n",
    "model.save(model_dir+'nvidia_suggested_model_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirt roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dirt, y_train_dirt = load_from_dir('data/recorded_data/lap1_dirt_bridge/', discard_prob=0, load_cached=False)\n",
    "# model = keras.models.load_model('data/sample_data/'+'nvidia_suggested_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train_dirt[2200])\n",
    "print(y_train_dirt[2200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr.assign(0.0001)\n",
    "model.fit(X_train_dirt, y_train_dirt, validation_split=0.1, shuffle=True, epochs=7)\n",
    "\n",
    "model.save(model_dir+'nvidia_suggested_model_dirt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr.assign(0.00005)\n",
    "model.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs=2)\n",
    "\n",
    "model.save(model_dir+'nvidia_suggested_model_dirt2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select images from each bucket for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[2])\n",
    "plt.imshow(X_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_from_dir('data/recorded_data/lap1/', [0, 1, 2], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs=7)\n",
    "\n",
    "model.save(model_dir+'nvidia_suggested_model_self_lap1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lap1 Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_from_dir('data/recorded_data/lap1mistakes/', [0, 1, 2], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs=6)\n",
    "\n",
    "model.save(model_dir+'nvidia_suggested_model_self_lap1mistakes.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lap1 Reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_from_dir('data/recorded_data/lap1reverse/', [0, 1, 2], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.optimizer.lr = 0.001\n",
    "model.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs=5)\n",
    "\n",
    "model.save(model_dir+'nvidia_suggested_model_self_lap1reverse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
